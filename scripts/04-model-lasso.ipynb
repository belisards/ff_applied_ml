{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, TransformedTargetRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, brier_score_loss, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import shap\n",
    "from skopt import BayesSearchCV\n",
    "import skopt.space as space\n",
    "import skopt.plots as plots\n",
    "import imblearn\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ff_custom_scripts import *\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, target, test, leaderboard, holdout, classifier=False):\n",
    "    # Get CV score\n",
    "    score = -model.best_score_\n",
    "    print(f'Best CV score: {score:.4f}')\n",
    "    \n",
    "    # Get mean CV score\n",
    "    mean_score = -model.cv_results_['mean_test_score'].mean()\n",
    "    print(f'Mean CV score: {mean_score:.4f}')\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test, y_test = prepare_data(test, target)\n",
    "\n",
    "    if classifier:\n",
    "        # Compute test scores\n",
    "        y_pred = model.predict(X_test)\n",
    "        brier = brier_score_loss(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred.round())\n",
    "        \n",
    "        # Print test scores\n",
    "        print(f'Test Brier: {brier:.4f}')\n",
    "        print(f'Test F1: {f1:.4f}')\n",
    "\n",
    "        # # Compute leaderboard scores\n",
    "        X_leaderboard, y_leaderboard = prepare_data(leaderboard, target)\n",
    "        y_pred = model.predict(X_leaderboard)\n",
    "        brier = brier_score_loss(y_leaderboard, y_pred)\n",
    "        f1 = f1_score(y_leaderboard, y_pred.round())\n",
    "\n",
    "        # # Print leaderboard scores\n",
    "        print(f'Leaderboard Brier: {brier:.4f}')\n",
    "        print(f'Leaderboard F1: {f1:.4f}')\n",
    "\n",
    "        # # # Compute holdout scores\n",
    "        # X_holdout, y_holdout = prepare_data(holdout, target)\n",
    "        # y_pred = model.predict(X_holdout)\n",
    "        # y_holdout = y_holdout.astype(int)\n",
    "        # brier = brier_score_loss(y_holdout, y_pred)\n",
    "        # print(f'Holdout Brier: {brier:.4f}')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # Compute test scores\n",
    "        mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "        rsquared = r2_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        # Print test scores\n",
    "        print(f'Test MSE: {mse:.4f}')\n",
    "        print(f'Test R2: {rsquared:.4f}')\n",
    "\n",
    "        # # Compute leaderboard scores\n",
    "        X_leaderboard, y_leaderboard = prepare_data(leaderboard, target)\n",
    "        mse = mean_squared_error(y_leaderboard, model.predict(X_leaderboard))\n",
    "        rsquared = r2_score(y_leaderboard, model.predict(X_leaderboard))\n",
    "\n",
    "        # Print leaderboard scores\n",
    "        print(f'Leaderboard MSE: {mse:.4f}')\n",
    "        print(f'Leaderboard R2: {rsquared:.4f}')\n",
    "\n",
    "        # # # Compute holdout scores\n",
    "        # X_holdout, y_holdout = prepare_data(holdout, target)\n",
    "        # # X_holdout_transformed = model.best_estimator_.named_steps['preprocessor'].transform(X_holdout)\n",
    "        # mse = mean_squared_error(y_holdout, model.predict(X_holdout))\n",
    "        # # rsquared = r2_score(y_holdout, model.predict(X_holdout))\n",
    "\n",
    "        # # Print holdout scores\n",
    "        # print(f'Holdout MSE: {mse:.4f}')\n",
    "        # print(f'Holdout R2: {rsquared:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1172, 10396) (294, 10396) (530, 10396) (803, 10396)\n"
     ]
    }
   ],
   "source": [
    "train, test,leaderboard,holdout = load_files(nanvalues='remove')\n",
    "\n",
    "alldata = pd.concat([train, test]) # all data available for training in the FF Challenge\n",
    "\n",
    "print(train.shape, test.shape,leaderboard.shape,holdout.shape)\n",
    "\n",
    "meta = pd.read_csv('../metadata/metadata.csv', index_col=0)\n",
    "\n",
    "targets = ['gpa','grit','materialHardship','eviction','layoff','jobTraining']\n",
    "\n",
    "predictors = {target: list(meta[meta[target] != 0].index) for target in targets}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')), \n",
    "]\n",
    "    )\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ])\n",
    "\n",
    "ordered_transformer = Pipeline(steps=[\n",
    "        ('target', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1,encoded_missing_value=-1)),\n",
    "    ])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train, target='gpa', classifier=False):\n",
    "    X_train, y_train = prepare_data(train, target)\n",
    "\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    numerical_features = cols_per_type(X_train, 'Continuous')\n",
    "    categorical_features = cols_per_type(X_train, 'Categorical')\n",
    "    binary_features = cols_per_type(X_train, 'Binary')\n",
    "    ordinal_features = cols_per_type(X_train, 'Ordinal')\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (numerical_transformer, numerical_features),\n",
    "        (ordered_transformer, ordinal_features),\n",
    "        (ordered_transformer, binary_features)\n",
    "    )\n",
    "\n",
    "    search_space = {\n",
    "        'regressor__max_iter': space.Integer(8_000, 20_000),\n",
    "    }\n",
    "    \n",
    "    if classifier:\n",
    "        model = LogisticRegression(penalty='l2')\n",
    "        score = 'neg_brier_score'\n",
    "        pipes = imbPipeline(steps=[('preprocessor', preprocessor),\n",
    "                                      ('smote', SMOTE(random_state=42)),\n",
    "                            ('regressor', model)])\n",
    "           \n",
    "    else:\n",
    "        model = Lasso()\n",
    "        score = 'neg_mean_squared_error'\n",
    "        search_space.update({\n",
    "            'regressor__alpha': space.Real(800, 1500),\n",
    "        })\n",
    "\n",
    "        pipes = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('regressor', model)])\n",
    "    \n",
    "    \n",
    "                                           \n",
    "    model = BayesSearchCV(\n",
    "        pipes,\n",
    "        search_space,\n",
    "        n_iter=10,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=score,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_model = run_model(train,target='gpa', classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.4355\n",
      "Mean CV score: 0.4355\n",
      "Test MSE: 0.4802\n",
      "Test R2: -0.0071\n",
      "Leaderboard MSE: 0.3939\n",
      "Leaderboard R2: -0.0084\n"
     ]
    }
   ],
   "source": [
    "score_model(gpa_model,'gpa',test,leaderboard,holdout,classifier=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Hardship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_materialHardship = run_model(train,target='materialHardship', classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regressor__alpha', 1087.0727711973198),\n",
       "             ('regressor__max_iter', 16733)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_materialHardship.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.0240\n",
      "Mean CV score: 0.0240\n",
      "Test MSE: 0.0253\n",
      "Test R2: -0.0000\n",
      "Leaderboard MSE: 0.0288\n",
      "Leaderboard R2: -0.0070\n"
     ]
    }
   ],
   "source": [
    "score_model(model_materialHardship, target='materialHardship', test=test, leaderboard=leaderboard, holdout=holdout, classifier=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grit = run_model(train,target='grit', classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regressor__alpha', 1087.0727711973198),\n",
       "             ('regressor__max_iter', 16733)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.2363\n",
      "Mean CV score: 0.2363\n",
      "Test MSE: 0.2321\n",
      "Test R2: -0.0047\n",
      "Leaderboard MSE: 0.2202\n",
      "Leaderboard R2: -0.0022\n"
     ]
    }
   ],
   "source": [
    "score_model(model_grit, target='grit', test=test, leaderboard=leaderboard, holdout=holdout, classifier=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eviction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eviction = run_model(train,target='eviction', classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.0739\n",
      "Mean CV score: 0.0739\n",
      "Test Brier: 0.0856\n",
      "Test F1: 0.0741\n",
      "Leaderboard Brier: 0.0925\n",
      "Leaderboard F1: 0.0755\n"
     ]
    }
   ],
   "source": [
    "score_model(model_eviction,'eviction', test, leaderboard, holdout,classifier=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jobTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_job = run_model(train,target='jobTraining', classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.3105\n",
      "Mean CV score: 0.3105\n",
      "Test Brier: 0.3219\n",
      "Test F1: 0.2769\n",
      "Leaderboard Brier: 0.3660\n",
      "Leaderboard F1: 0.2240\n"
     ]
    }
   ],
   "source": [
    "score_model(model_job, 'jobTraining', test, leaderboard, holdout,classifier=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layoff = run_model(train, target='layoff', classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regressor__max_iter', 12921)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layoff.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.2641\n",
      "Mean CV score: 0.2641\n",
      "Test Brier: 0.3566\n",
      "Test F1: 0.1930\n",
      "Leaderboard Brier: 0.3151\n",
      "Leaderboard F1: 0.2707\n"
     ]
    }
   ],
   "source": [
    "score_model(model_layoff, 'layoff', test, leaderboard, holdout, classifier=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
