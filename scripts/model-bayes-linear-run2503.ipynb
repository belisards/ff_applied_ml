{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, TransformedTargetRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, brier_score_loss, f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import shap\n",
    "from skopt import BayesSearchCV\n",
    "import skopt.space as space\n",
    "import skopt.plots as plots\n",
    "import imblearn\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from ff_custom_scripts import *\n",
    "# import RandomForestRegressor and selectfrom model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# import knn imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "# from sklearn import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model(model, target, test, leaderboard, holdout, classifier=False):\n",
    "    # Get CV score\n",
    "    score = -model.best_score_\n",
    "    print(f'Best CV score: {score:.4f}')\n",
    "    \n",
    "    # Get mean CV score\n",
    "    mean_score = -model.cv_results_['mean_test_score'].mean()\n",
    "    print(f'Mean CV score: {mean_score:.4f}')\n",
    "    \n",
    "    # Prepare test data\n",
    "    X_test, y_test = prepare_data(test, target)\n",
    "\n",
    "    if classifier:\n",
    "        # Compute test scores\n",
    "        y_pred = model.predict(X_test)\n",
    "        brier = brier_score_loss(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred.round())\n",
    "        \n",
    "        # Print test scores\n",
    "        print(f'Test Brier: {brier:.4f}')\n",
    "        print(f'Test F1: {f1:.4f}')\n",
    "\n",
    "        # # Compute leaderboard scores\n",
    "        X_leaderboard, y_leaderboard = prepare_data(leaderboard, target)\n",
    "        y_pred = model.predict(X_leaderboard)\n",
    "        brier = brier_score_loss(y_leaderboard, y_pred)\n",
    "        f1 = f1_score(y_leaderboard, y_pred.round())\n",
    "\n",
    "        # # Print leaderboard scores\n",
    "        print(f'Leaderboard Brier: {brier:.4f}')\n",
    "        print(f'Leaderboard F1: {f1:.4f}')\n",
    "\n",
    "        # # Compute holdout scores\n",
    "        X_holdout, y_holdout = prepare_data(holdout, target)\n",
    "        y_pred = model.predict(X_holdout)\n",
    "        y_holdout = y_holdout.astype(int)\n",
    "        brier = brier_score_loss(y_holdout, y_pred)\n",
    "        print(f'Holdout Brier: {brier:.4f}')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        # Compute test scores\n",
    "        mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "        rsquared = r2_score(y_test, model.predict(X_test))\n",
    "        \n",
    "        # Print test scores\n",
    "        print(f'Test MSE: {mse:.4f}')\n",
    "        print(f'Test R2: {rsquared:.4f}')\n",
    "\n",
    "        # # Compute leaderboard scores\n",
    "        X_leaderboard, y_leaderboard = prepare_data(leaderboard, target)\n",
    "        mse = mean_squared_error(y_leaderboard, model.predict(X_leaderboard))\n",
    "        rsquared = r2_score(y_leaderboard, model.predict(X_leaderboard))\n",
    "\n",
    "        # Print leaderboard scores\n",
    "        print(f'Leaderboard MSE: {mse:.4f}')\n",
    "        print(f'Leaderboard R2: {rsquared:.4f}')\n",
    "\n",
    "        # # Compute holdout scores\n",
    "        X_holdout, y_holdout = prepare_data(holdout, target)\n",
    "        # X_holdout_transformed = model.best_estimator_.named_steps['preprocessor'].transform(X_holdout)\n",
    "        mse = mean_squared_error(y_holdout, model.predict(X_holdout))\n",
    "        rsquared = r2_score(y_holdout, model.predict(X_holdout))\n",
    "\n",
    "        # Print holdout scores\n",
    "        print(f'Holdout MSE: {mse:.4f}')\n",
    "        print(f'Holdout R2: {rsquared:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1172, 10396) (294, 10396) (530, 10396) (803, 10396)\n"
     ]
    }
   ],
   "source": [
    "train, test,leaderboard,holdout = load_files(nanvalues='remove')\n",
    "\n",
    "alldata = pd.concat([train, test]) # all data available for training in the FF Challenge\n",
    "\n",
    "print(train.shape, test.shape,leaderboard.shape,holdout.shape)\n",
    "\n",
    "meta = pd.read_csv('../metadata/metadata.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "challengeID\n",
       "4       False\n",
       "11       True\n",
       "15      False\n",
       "17       True\n",
       "29      False\n",
       "        ...  \n",
       "4223    False\n",
       "4226    False\n",
       "4228    False\n",
       "4230    False\n",
       "4237    False\n",
       "Name: layoff, Length: 803, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_data(holdout,'layoff')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varlab</th>\n",
       "      <th>type</th>\n",
       "      <th>one_topic</th>\n",
       "      <th>dtype</th>\n",
       "      <th>gpa</th>\n",
       "      <th>grit</th>\n",
       "      <th>materialHardship</th>\n",
       "      <th>eviction</th>\n",
       "      <th>layoff</th>\n",
       "      <th>jobTraining</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cf1lenhr</th>\n",
       "      <td>What was the total length of interview - Hours</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>paradata_and_weights</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cf1lenmin</th>\n",
       "      <td>What was the total length of interview - Minutes</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>paradata_and_weights</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cf1fint</th>\n",
       "      <td>Constructed - Was father interviewed at baseline?</td>\n",
       "      <td>Binary</td>\n",
       "      <td>paradata_and_weights</td>\n",
       "      <td>object</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.018859</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>0.028699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cf1citsm</th>\n",
       "      <td>Constructed - Baseline city sample flag</td>\n",
       "      <td>Binary</td>\n",
       "      <td>paradata_and_weights</td>\n",
       "      <td>object</td>\n",
       "      <td>0.012485</td>\n",
       "      <td>0.023336</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016590</td>\n",
       "      <td>0.004293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1citywt</th>\n",
       "      <td>Father baseline city sample weight (20-cities ...</td>\n",
       "      <td>Continuous</td>\n",
       "      <td>paradata_and_weights</td>\n",
       "      <td>float64</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009234</td>\n",
       "      <td>0.002574</td>\n",
       "      <td>0.014284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      varlab        type  \\\n",
       "new_name                                                                   \n",
       "cf1lenhr      What was the total length of interview - Hours  Continuous   \n",
       "cf1lenmin   What was the total length of interview - Minutes  Continuous   \n",
       "cf1fint    Constructed - Was father interviewed at baseline?      Binary   \n",
       "cf1citsm             Constructed - Baseline city sample flag      Binary   \n",
       "f1citywt   Father baseline city sample weight (20-cities ...  Continuous   \n",
       "\n",
       "                      one_topic    dtype       gpa      grit  \\\n",
       "new_name                                                       \n",
       "cf1lenhr   paradata_and_weights  float64  0.000000  0.009892   \n",
       "cf1lenmin  paradata_and_weights  float64  0.000000  0.000000   \n",
       "cf1fint    paradata_and_weights   object  0.021894  0.018859   \n",
       "cf1citsm   paradata_and_weights   object  0.012485  0.023336   \n",
       "f1citywt   paradata_and_weights  float64  0.000000  0.000000   \n",
       "\n",
       "           materialHardship  eviction    layoff  jobTraining  \n",
       "new_name                                                      \n",
       "cf1lenhr           0.003785  0.000000  0.007810     0.000000  \n",
       "cf1lenmin          0.000000  0.001409  0.000000     0.000000  \n",
       "cf1fint            0.016743  0.028699  0.000000     0.000000  \n",
       "cf1citsm           0.011364  0.000000  0.016590     0.004293  \n",
       "f1citywt           0.000000  0.009234  0.002574     0.014284  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['gpa','grit','materialHardship','eviction','layoff','jobTraining']\n",
    "\n",
    "predictors = {target: list(meta[meta[target] != 0].index) for target in targets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')), \n",
    "    ('variance', VarianceThreshold(threshold=0.1)),\n",
    "]\n",
    "    )\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('variance', VarianceThreshold(threshold=0.1)),\n",
    "    ])\n",
    "\n",
    "ordered_transformer = Pipeline(steps=[\n",
    "        # ('target', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)),\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('variance', VarianceThreshold(threshold=0.1)),\n",
    "    ])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(train, target='gpa', classifier=False):\n",
    "    X_train, y_train = prepare_data(train, target)\n",
    "    \n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    numerical_features = cols_per_type(X_train, 'Continuous')\n",
    "    categorical_features = cols_per_type(X_train, 'Categorical')\n",
    "    binary_features = cols_per_type(X_train, 'Binary')\n",
    "    ordinal_features = cols_per_type(X_train, 'Ordinal')\n",
    "\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    preprocessor = make_column_transformer(\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (numerical_transformer, numerical_features),\n",
    "        (ordered_transformer, ordinal_features),\n",
    "        (ordered_transformer, binary_features)\n",
    "    )\n",
    "\n",
    "    search_space = {\n",
    "        'regressor__max_iter': space.Integer(8_000, 20_000),\n",
    "    }\n",
    "    \n",
    "    if classifier:\n",
    "        classifier = LogisticRegression(penalty='l2')\n",
    "        score = 'neg_brier_score'\n",
    "           \n",
    "    else:\n",
    "        classifier = Lasso()\n",
    "        score = 'neg_mean_squared_error'\n",
    "        search_space.update({\n",
    "            'regressor__alpha': space.Real(800, 2000),\n",
    "        })\n",
    "    \n",
    "    pipes = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                            ('regressor', classifier)])\n",
    "                                           \n",
    "    model = BayesSearchCV(\n",
    "        pipes,\n",
    "        search_space,\n",
    "        n_iter=10,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        scoring=score,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    # X_cv = preprocessor.fit_transform(X_cv)\n",
    "\n",
    "    # model.fit(X_train, y_train, regressor__eval_set=[(X_cv, y_cv)], regressor__verbose=True)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_show(model, alldata, target, n=5):\n",
    "    X, y = prepare_data(alldata, target)\n",
    "    model  = model.best_estimator_.fit(X, y)\n",
    "    Xtransform = model.named_steps['preprocessor'].transform(X)\n",
    "    exp = shap.TreeExplainer(model.named_steps['regressor'])\n",
    "    transformer = model.named_steps['preprocessor']\n",
    "    names = transformer.get_feature_names_out()\n",
    "    featnames = [splitfeatname(name) for name in names]\n",
    "    shap_values = exp.shap_values(Xtransform)\n",
    "    # get top n features\n",
    "    top_n_idx = np.argsort(np.abs(shap_values).mean(0))[-n:]\n",
    "    top_n_feat = [featnames[i] for i in top_n_idx]\n",
    "    # # get questions\n",
    "    top_n_vars = [meta[meta.index.isin([feat])].varlab.values for feat in top_n_feat]\n",
    "    # # reverse order\n",
    "    top_n_vars = top_n_vars[::-1]\n",
    "    shap.summary_plot(shap_values, Xtransform, max_display=n, feature_names=featnames)\n",
    "    return dict(zip(map(tuple, top_n_vars), top_n_feat))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpa_model = run_model(train,target='gpa', classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_model(model, target, test, leaderboard, holdout, classifier=False):\n",
    "#     # Get CV score\n",
    "#     score = -model.best_score_\n",
    "#     print(f'Best CV score: {score:.4f}')\n",
    "    \n",
    "#     # Get mean CV score\n",
    "#     mean_score = -model.cv_results_['mean_test_score'].mean()\n",
    "#     print(f'Mean CV score: {mean_score:.4f}')\n",
    "    \n",
    "#     # Prepare test data\n",
    "#     X_test, y_test = prepare_data(test, target)\n",
    "\n",
    "#     if classifier:\n",
    "#         # Compute test scores\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         brier = brier_score_loss(y_test, y_pred)\n",
    "#         f1 = f1_score(y_test, y_pred.round())\n",
    "        \n",
    "#         # Print test scores\n",
    "#         print(f'Test Brier: {brier:.4f}')\n",
    "#         print(f'Test F1: {f1:.4f}')\n",
    "\n",
    "#         # # Compute leaderboard scores\n",
    "#         X_leaderboard, y_leaderboard = prepare_data(leaderboard, target)\n",
    "#         y_pred = model.predict(X_leaderboard)\n",
    "#         brier = brier_score_loss(y_leaderboard, y_pred)\n",
    "#         f1 = f1_score(y_leaderboard, y_pred.round())\n",
    "\n",
    "#         # # Print leaderboard scores\n",
    "#         print(f'Leaderboard Brier: {brier:.4f}')\n",
    "#         print(f'Leaderboard F1: {f1:.4f}')\n",
    "\n",
    "#         # # Compute holdout scores\n",
    "#         X_holdout, y_holdout = prepare_data(holdout, target)\n",
    "#         y_pred = model.predict(X_holdout)\n",
    "#         brier = brier_score_loss(y_holdout, y_pred)\n",
    "\n",
    "#         # Print holdout scores\n",
    "#         print(f'Holdout Brier: {brier:.4f}')\n",
    "        \n",
    "#     else:\n",
    "#         # Compute test scores\n",
    "#         mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "#         rsquared = r2_score(y_test, model.predict(X_test))\n",
    "        \n",
    "#         # Print test scores\n",
    "#         print(f'Test MSE: {mse:.4f}')\n",
    "#         print(f'Test R2: {rsquared:.4f}')\n",
    "\n",
    "#         # # Compute leaderboard scores\n",
    "#         X_leaderboard, y_leaderboard = prepare_data(leaderboard, target)\n",
    "#         mse = mean_squared_error(y_leaderboard, model.predict(X_leaderboard))\n",
    "#         rsquared = r2_score(y_leaderboard, model.predict(X_leaderboard))\n",
    "\n",
    "#         # Print leaderboard scores\n",
    "#         print(f'Leaderboard MSE: {mse:.4f}')\n",
    "#         print(f'Leaderboard R2: {rsquared:.4f}')\n",
    "\n",
    "#         # # Compute holdout scores\n",
    "#         X_holdout, y_holdout = prepare_data(holdout, target)\n",
    "#         mse = mean_squared_error(y_holdout, model.predict(X_holdout))\n",
    "#         rsquared = r2_score(y_holdout, model.predict(X_holdout))\n",
    "\n",
    "#         # Print holdout scores\n",
    "#         print(f'Holdout MSE: {mse:.4f}')\n",
    "#         print(f'Holdout R2: {rsquared:.4f}')\n",
    "        \n",
    "# score_model(gpa_model,'gpa',test,leaderboard,holdout,classifier=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Material Hardship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_materialHardship = run_model(train,target='materialHardship', classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regressor__alpha', 1292.1247506239765),\n",
       "             ('regressor__max_iter', 16733)])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrderedDict([('regressor__gamma', 0.17995534416071732),\n",
    "#              ('regressor__learning_rate', 0.04942262677968311),\n",
    "#              ('regressor__max_depth', 8),\n",
    "#              ('regressor__n_estimators', 1001)])\n",
    "\n",
    "# OrderedDict([('regressor__gamma', 0.18373883555532844),\n",
    "#              ('regressor__learning_rati e', 0.07183206941666036),\n",
    "#              ('regressor__max_depth', 7),\n",
    "#              ('regressor__min_child_weight', 5),\n",
    "#              ('regressor__n_estimators', 1105)])\n",
    "\n",
    "model_materialHardship.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.0240\n",
      "Mean CV score: 0.0240\n",
      "Test MSE: 0.0253\n",
      "Test R2: -0.0000\n",
      "Leaderboard MSE: 0.0288\n",
      "Leaderboard R2: -0.0070\n",
      "Holdout MSE: 0.0244\n",
      "Holdout R2: -0.0017\n"
     ]
    }
   ],
   "source": [
    "score_model(model_materialHardship, target='materialHardship', test=test, leaderboard=leaderboard, holdout=holdout, classifier=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grit = run_model(train,target='grit', classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regressor__alpha', 1292.1247506239765),\n",
       "             ('regressor__max_iter', 16733)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_grit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.2363\n",
      "Mean CV score: 0.2363\n",
      "Test MSE: 0.2321\n",
      "Test R2: -0.0047\n",
      "Leaderboard MSE: 0.2202\n",
      "Leaderboard R2: -0.0022\n",
      "Holdout MSE: 0.2394\n",
      "Holdout R2: -0.0020\n"
     ]
    }
   ],
   "source": [
    "score_model(model_grit, target='grit', test=test, leaderboard=leaderboard, holdout=holdout, classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varlab</th>\n",
       "      <th>type</th>\n",
       "      <th>one_topic</th>\n",
       "      <th>dtype</th>\n",
       "      <th>gpa</th>\n",
       "      <th>grit</th>\n",
       "      <th>materialHardship</th>\n",
       "      <th>eviction</th>\n",
       "      <th>layoff</th>\n",
       "      <th>jobTraining</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m5a8f01</th>\n",
       "      <td>A8F. Your relationship with the father at biol...</td>\n",
       "      <td>Ordered Categorical</td>\n",
       "      <td>romantic_relationships</td>\n",
       "      <td>category</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026602</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     varlab  \\\n",
       "new_name                                                      \n",
       "m5a8f01   A8F. Your relationship with the father at biol...   \n",
       "\n",
       "                         type               one_topic     dtype  gpa  grit  \\\n",
       "new_name                                                                     \n",
       "m5a8f01   Ordered Categorical  romantic_relationships  category  0.0   0.0   \n",
       "\n",
       "          materialHardship  eviction  layoff  jobTraining  \n",
       "new_name                                                   \n",
       "m5a8f01           0.026602       0.0     0.0          0.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[meta.index=='m5a8f01']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eviction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eviction = run_model(train,target='eviction', classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.0721\n",
      "Mean CV score: 0.0721\n",
      "Test Brier: 0.0890\n",
      "Test F1: 0.0714\n",
      "Leaderboard Brier: 0.1019\n",
      "Leaderboard F1: 0.0357\n",
      "Holdout Brier: 0.1071\n"
     ]
    }
   ],
   "source": [
    "score_model(model_eviction,'eviction', test, leaderboard, holdout,classifier=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jobTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_job = run_model(train,target='jobTraining', classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.3075\n",
      "Mean CV score: 0.3075\n",
      "Test Brier: 0.3390\n",
      "Test F1: 0.2205\n",
      "Leaderboard Brier: 0.3792\n",
      "Leaderboard F1: 0.1862\n",
      "Holdout Brier: 0.3711\n"
     ]
    }
   ],
   "source": [
    "score_model(model_job, 'jobTraining', test, leaderboard, holdout,classifier=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layoff = run_model(train, target='layoff', classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regressor__max_iter', 12921)])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_layoff.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.2715\n",
      "Mean CV score: 0.2715\n",
      "Test Brier: 0.3605\n",
      "Test F1: 0.1913\n",
      "Leaderboard Brier: 0.3264\n",
      "Leaderboard F1: 0.2700\n",
      "Holdout Brier: 0.3537\n"
     ]
    }
   ],
   "source": [
    "score_model(model_layoff, 'layoff', test, leaderboard, holdout, classifier=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
